{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RPT - Teste de Esparsidade no BitNet 2B (Microsoft)\n",
    "#\n",
    "# Objetivo: validar se esparsidade melhora/mantem qualidade em modelo ternario.\n",
    "# Base: RPT_BitNet_Microsoft.ipynb (que ja funciona).\n",
    "#\n",
    "# IMPORTANTE: Rode Cell 1, depois REINICIE o runtime, depois rode Cell 1 de novo e continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: SETUP (identico ao notebook Microsoft que funcionou)\n",
    "# Na primeira vez, o pip install roda. Reinicie o runtime e rode esta cell de novo.\n",
    "!pip install -q torch torchvision\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git accelerate\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import transformers\n",
    "\n",
    "print('Transformers:', transformers.__version__)\n",
    "print('Torch:', torch.__version__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "if device.type == 'cuda':\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "    mem = torch.cuda.get_device_properties(0).total_memory\n",
    "    print('VRAM: {:.1f} GB'.format(mem / 1e9))\n",
    "    print('BF16 suportado:', torch.cuda.is_bf16_supported())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: CARREGAR MODELO (identico ao notebook Microsoft que funcionou)\n",
    "MODEL_ID = 'microsoft/bitnet-b1.58-2B-4T-bf16'\n",
    "\n",
    "print('Carregando tokenizer...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "print('Carregando modelo BitNet 2B...')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map='auto'\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print('Parametros: {:,.0f} ({:.1f}B)'.format(n_params, n_params / 1e9))\n",
    "if device.type == 'cuda':\n",
    "    print('VRAM usada: {:.1f} GB'.format(torch.cuda.memory_allocated() / 1e9))\n",
    "print('Pronto!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: ANALISAR DISTRIBUICAO DOS PESOS\n",
    "# Antes de aplicar esparsidade, entender o que temos\n",
    "\n",
    "print('=== DISTRIBUICAO DOS PESOS ===')\n",
    "print()\n",
    "\n",
    "total_params = 0\n",
    "total_zeros = 0\n",
    "total_near_zero = 0\n",
    "weight_layers = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.dim() < 2:\n",
    "        continue  # pular biases e norms\n",
    "    w = param.data.float()\n",
    "    n = w.numel()\n",
    "    zeros = (w == 0).sum().item()\n",
    "    near_zero = (w.abs() < 0.01).sum().item()\n",
    "    total_params += n\n",
    "    total_zeros += zeros\n",
    "    total_near_zero += near_zero\n",
    "    weight_layers.append((name, n, zeros, w.abs().mean().item()))\n",
    "\n",
    "print('Total parametros (matrices): {:,.0f}'.format(total_params))\n",
    "print('Zeros exatos: {:,.0f} ({:.1f}%)'.format(total_zeros, 100 * total_zeros / total_params))\n",
    "print('Perto de zero (<0.01): {:,.0f} ({:.1f}%)'.format(total_near_zero, 100 * total_near_zero / total_params))\n",
    "print()\n",
    "\n",
    "# Mostrar distribuicao de uma camada exemplo\n",
    "print('--- Exemplo: primeiras 5 camadas de peso ---')\n",
    "for name, n, zeros, mean_abs in weight_layers[:5]:\n",
    "    print('  {} | params={:,} | zeros={:.1f}% | abs_mean={:.6f}'.format(\n",
    "        name[:60], n, 100 * zeros / n, mean_abs))\n",
    "\n",
    "# Valores unicos de uma camada\n",
    "print()\n",
    "print('--- Valores unicos (primeira camada linear) ---')\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name and param.dim() == 2 and 'embed' not in name:\n",
    "        w = param.data.float().flatten()\n",
    "        uniques = torch.unique(w)\n",
    "        if len(uniques) <= 20:\n",
    "            print('  {} tem {} valores unicos: {}'.format(\n",
    "                name[:50], len(uniques), uniques.tolist()))\n",
    "        else:\n",
    "            print('  {} tem {} valores unicos (min={:.4f}, max={:.4f})'.format(\n",
    "                name[:50], len(uniques), w.min().item(), w.max().item()))\n",
    "        break\n",
    "\n",
    "print()\n",
    "print('Esparsidade NATURAL do modelo: {:.1f}%'.format(100 * total_zeros / total_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: FUNCOES DE TESTE\n",
    "# Mesmas funcoes do notebook Microsoft + perplexity\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    'The capital of France is',\n",
    "    'Water boils at',\n",
    "    'The largest planet in the solar system is',\n",
    "    'Python is a programming language that',\n",
    "    'In 1969, humans first',\n",
    "]\n",
    "\n",
    "EVAL_TEXTS = [\n",
    "    'The capital of France is Paris, which is known for the Eiffel Tower.',\n",
    "    'Water boils at 100 degrees Celsius at standard atmospheric pressure.',\n",
    "    'The largest planet in the solar system is Jupiter, a gas giant.',\n",
    "    'Python is a programming language that is widely used for data science.',\n",
    "    'In 1969, humans first landed on the Moon during the Apollo 11 mission.',\n",
    "    'Machine learning is a subset of artificial intelligence that focuses on learning from data.',\n",
    "    'The speed of light in vacuum is approximately 299,792,458 meters per second.',\n",
    "    'DNA contains the genetic instructions for the development of all living organisms.',\n",
    "]\n",
    "\n",
    "def compute_perplexity(model, tokenizer, texts):\n",
    "    \"\"\"Calcula perplexity media sobre uma lista de textos.\"\"\"\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=128)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "        n_tokens = inputs['input_ids'].shape[1] - 1\n",
    "        total_loss += outputs.loss.item() * n_tokens\n",
    "        total_tokens += n_tokens\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    return torch.exp(torch.tensor(avg_loss)).item()\n",
    "\n",
    "def test_generation(model, tokenizer, prompts):\n",
    "    \"\"\"Testa geracao e retorna textos.\"\"\"\n",
    "    results = []\n",
    "    for prompt in prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=30,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        results.append(text)\n",
    "    return results\n",
    "\n",
    "def chat(user_msg, system_msg='You are a helpful AI assistant.', max_tokens=200):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_msg},\n",
    "        {'role': 'user', 'content': user_msg},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    input_len = inputs['input_ids'].shape[-1]\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(out[0][input_len:], skip_special_tokens=True)\n",
    "\n",
    "print('Funcoes de teste definidas.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5: BASELINE (0% esparsidade adicional)\n",
    "\n",
    "print('=== BASELINE: MODELO ORIGINAL (0% esparsidade adicional) ===')\n",
    "print()\n",
    "\n",
    "ppl_baseline = compute_perplexity(model, tokenizer, EVAL_TEXTS)\n",
    "print('Perplexity baseline: {:.2f}'.format(ppl_baseline))\n",
    "print()\n",
    "\n",
    "print('--- Geracao ---')\n",
    "baseline_texts = test_generation(model, tokenizer, TEST_PROMPTS)\n",
    "for text in baseline_texts:\n",
    "    print('  {}'.format(text))\n",
    "print()\n",
    "\n",
    "print('--- Chat ---')\n",
    "resp = chat('What is the capital of France?')\n",
    "print('  Q: What is the capital of France?')\n",
    "print('  A: {}'.format(resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 6: APLICAR ESPARSIDADE PROGRESSIVA\n# v3: com debug detalhado para acompanhar progresso\n\nimport time\n\n# Salvar estado original dos pesos NA CPU\nprint('Salvando pesos originais na CPU...')\noriginal_state = {}\nfor name, param in model.named_parameters():\n    if param.dim() >= 2 and 'embed' not in name and 'lm_head' not in name:\n        original_state[name] = param.data.cpu().clone()\nprint('Salvos {} tensores de peso.'.format(len(original_state)))\n\ntorch.cuda.empty_cache()\nif device.type == 'cuda':\n    print('VRAM apos salvar: {:.1f} GB'.format(torch.cuda.memory_allocated() / 1e9))\n\n\ndef apply_sparsity(model, sparsity_pct, original_state, verbose=False):\n    \"\"\"Aplica esparsidade por magnitude usando amostragem para o threshold.\"\"\"\n    if sparsity_pct <= 0:\n        if verbose:\n            print('  Restaurando pesos originais...')\n        for name, param in model.named_parameters():\n            if name in original_state:\n                param.data.copy_(original_state[name].to(param.device))\n        if verbose:\n            print('  Restaurado.')\n        return 0.0\n\n    # PASSO 1: Amostrar magnitudes na CPU\n    if verbose:\n        print('  [1/3] Amostrando magnitudes de {} camadas...'.format(len(original_state)))\n    t0 = time.time()\n    samples = []\n    total_nonzero = 0\n    for name in original_state:\n        w = original_state[name].float()\n        nonzero = w[w != 0].abs()\n        n = nonzero.numel()\n        if n == 0:\n            continue\n        total_nonzero += n\n        if n > 50000:\n            idx = torch.randperm(n)[:50000]\n            samples.append(nonzero[idx])\n        else:\n            samples.append(nonzero)\n\n    sample = torch.cat(samples)\n    if verbose:\n        print('  [1/3] Amostrados {:,} valores de {:,} nao-zeros ({:.1f}s)'.format(\n            len(sample), total_nonzero, time.time() - t0))\n\n    # PASSO 2: Calcular threshold\n    if verbose:\n        print('  [2/3] Calculando threshold para {}%...'.format(sparsity_pct))\n    t1 = time.time()\n    sample_sorted = sample.sort().values\n    idx = int(len(sample_sorted) * sparsity_pct / 100.0)\n    idx = min(idx, len(sample_sorted) - 1)\n    threshold = sample_sorted[idx].item()\n    del sample, sample_sorted, samples\n    if verbose:\n        print('  [2/3] Threshold: {:.6f} ({:.1f}s)'.format(threshold, time.time() - t1))\n\n    # PASSO 3: Aplicar mascara camada por camada\n    if verbose:\n        print('  [3/3] Aplicando mascara em {} camadas...'.format(len(original_state)))\n    t2 = time.time()\n    total_pruned = 0\n    total_weights = 0\n    count = 0\n    for name, param in model.named_parameters():\n        if name not in original_state:\n            continue\n        w = original_state[name].clone()\n        mask = w.float().abs() > threshold\n        w[~mask] = 0\n        param.data.copy_(w.to(param.device))\n        total_pruned += (~mask).sum().item()\n        total_weights += w.numel()\n        count += 1\n        if verbose and count % 50 == 0:\n            print('    ... {}/{} camadas'.format(count, len(original_state)))\n\n    actual = 100.0 * total_pruned / total_weights\n    if verbose:\n        print('  [3/3] Feito: {:,} de {:,} zerados ({:.1f}%) em {:.1f}s'.format(\n            total_pruned, total_weights, actual, time.time() - t2))\n    return actual\n\n\nprint('Funcao de esparsidade pronta.')\nprint()\nprint('Teste rapido: aplicar 30% com debug...')\ns = apply_sparsity(model, 30, original_state, verbose=True)\nprint()\nprint('Restaurando...')\napply_sparsity(model, 0, original_state, verbose=True)\nprint('OK!')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: TESTE PROGRESSIVO DE ESPARSIDADE\n",
    "# Testar em: 0%, 10%, 20%, 30%, 50%, 70%, 80%, 90%\n",
    "\n",
    "sparsity_levels = [0, 10, 20, 30, 50, 70, 80, 90]\n",
    "results = []\n",
    "\n",
    "print('=== TESTE PROGRESSIVO DE ESPARSIDADE ===')\n",
    "print('Testando {} niveis...'.format(len(sparsity_levels)))\n",
    "print()\n",
    "\n",
    "for target_sp in sparsity_levels:\n",
    "    print('--- Esparsidade: {}% ---'.format(target_sp))\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Aplicar esparsidade (restaura originais primeiro)\n",
    "    actual_sp = apply_sparsity(model, target_sp, original_state)\n",
    "\n",
    "    # Medir perplexity\n",
    "    ppl = compute_perplexity(model, tokenizer, EVAL_TEXTS)\n",
    "\n",
    "    # Testar geracao (3 prompts pra ser rapido)\n",
    "    texts = test_generation(model, tokenizer, TEST_PROMPTS[:3])\n",
    "\n",
    "    dt = time.time() - t0\n",
    "\n",
    "    # Avaliar qualidade do texto\n",
    "    coherent = True\n",
    "    for t in texts:\n",
    "        words = t.split()\n",
    "        if len(words) > 3:\n",
    "            # Detectar repeticao excessiva\n",
    "            last_words = words[-6:]\n",
    "            if len(set(last_words)) <= 2:\n",
    "                coherent = False\n",
    "                break\n",
    "\n",
    "    result = {\n",
    "        'target_sparsity': target_sp,\n",
    "        'actual_sparsity': actual_sp,\n",
    "        'perplexity': ppl,\n",
    "        'coherent': coherent,\n",
    "        'sample_text': texts[0],\n",
    "        'time_sec': dt,\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    status = 'OK' if coherent else 'DEGRADOU'\n",
    "    print('  Esparsidade real: {:.1f}% | PPL: {:.2f} | {} | {:.1f}s'.format(\n",
    "        actual_sp, ppl, status, dt))\n",
    "    print('  Exemplo: {}'.format(texts[0][:100]))\n",
    "    print()\n",
    "\n",
    "# Restaurar modelo original\n",
    "apply_sparsity(model, 0, original_state)\n",
    "print('Modelo restaurado ao original.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8: TABELA DE RESULTADOS\n",
    "\n",
    "print('=' * 70)\n",
    "print('RESULTADOS: ESPARSIDADE NO BITNET 2B')\n",
    "print('=' * 70)\n",
    "print()\n",
    "print('{:<12} {:<12} {:<10} {:<10} {}'.format(\n",
    "    'Esparsidade', 'Real', 'PPL', 'Status', 'Exemplo'))\n",
    "print('-' * 70)\n",
    "\n",
    "for r in results:\n",
    "    status = 'OK' if r['coherent'] else 'QUEBROU'\n",
    "    sample = r['sample_text'][:40].replace('\\n', ' ')\n",
    "    print('{:<12} {:<12} {:<10} {:<10} {}'.format(\n",
    "        '{}%'.format(r['target_sparsity']),\n",
    "        '{:.1f}%'.format(r['actual_sparsity']),\n",
    "        '{:.2f}'.format(r['perplexity']),\n",
    "        status,\n",
    "        sample))\n",
    "\n",
    "print()\n",
    "print('--- ANALISE ---')\n",
    "baseline_ppl = results[0]['perplexity']\n",
    "print('PPL baseline (0%): {:.2f}'.format(baseline_ppl))\n",
    "\n",
    "# Achar limite de esparsidade (onde PPL dobra ou texto quebra)\n",
    "best_sparse = None\n",
    "for r in results:\n",
    "    if r['coherent'] and r['perplexity'] < baseline_ppl * 1.5:\n",
    "        best_sparse = r\n",
    "\n",
    "if best_sparse and best_sparse['target_sparsity'] > 0:\n",
    "    print('Melhor esparsidade com qualidade: {}% (PPL {:.2f}, {:.1f}% do baseline)'.format(\n",
    "        best_sparse['target_sparsity'],\n",
    "        best_sparse['perplexity'],\n",
    "        100 * best_sparse['perplexity'] / baseline_ppl))\n",
    "    print()\n",
    "    print('CONCLUSAO: BitNet 2B tolera {}% de esparsidade adicional!'.format(\n",
    "        best_sparse['target_sparsity']))\n",
    "else:\n",
    "    print('CONCLUSAO: Esparsidade degrada qualidade rapidamente neste modelo.')\n",
    "\n",
    "# Verificar se esparsidade MELHORA (como vimos no RPT_BitNet_Projeto)\n",
    "improved = [r for r in results if r['perplexity'] < baseline_ppl and r['target_sparsity'] > 0]\n",
    "if improved:\n",
    "    best = min(improved, key=lambda x: x['perplexity'])\n",
    "    print()\n",
    "    print('*** ESPARSIDADE MELHOROU! ***')\n",
    "    print('Melhor: {}% esparsidade -> PPL {:.2f} (era {:.2f}, melhoria de {:.1f}%)'.format(\n",
    "        best['target_sparsity'],\n",
    "        best['perplexity'],\n",
    "        baseline_ppl,\n",
    "        100 * (1 - best['perplexity'] / baseline_ppl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9: TESTE DETALHADO DO MELHOR NIVEL\n",
    "# Aplica o melhor nivel de esparsidade e faz testes completos\n",
    "\n",
    "# Escolher o melhor nivel (mais esparso que ainda funciona)\n",
    "baseline_ppl = results[0]['perplexity']\n",
    "coherent_results = [r for r in results if r['coherent']]\n",
    "good = [r for r in coherent_results\n",
    "        if r['perplexity'] < baseline_ppl * 1.5 and r['target_sparsity'] > 0]\n",
    "\n",
    "if good:\n",
    "    best_level = max(good, key=lambda x: x['target_sparsity'])['target_sparsity']\n",
    "else:\n",
    "    best_level = 10  # fallback conservador\n",
    "\n",
    "print('=== TESTE DETALHADO: {}% ESPARSIDADE ==='.format(best_level))\n",
    "print()\n",
    "\n",
    "apply_sparsity(model, best_level, original_state)\n",
    "\n",
    "# Teste de geracao completo\n",
    "print('--- Completar frase ---')\n",
    "for prompt in TEST_PROMPTS:\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=30,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    print('  {}'.format(text))\n",
    "print()\n",
    "\n",
    "# Teste de chat\n",
    "print('--- Chat ---')\n",
    "perguntas_chat = [\n",
    "    'What is the capital of France?',\n",
    "    'Explain what BitNet is in 2 sentences.',\n",
    "    'Write a Python function that checks if a number is prime.',\n",
    "]\n",
    "for p in perguntas_chat:\n",
    "    print('User:', p)\n",
    "    print('BitNet ({}% sparse):'.format(best_level), chat(p))\n",
    "    print('-' * 60)\n",
    "\n",
    "# Restaurar\n",
    "apply_sparsity(model, 0, original_state)\n",
    "print()\n",
    "print('Modelo restaurado.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10: SALVAR RESULTADOS\n",
    "import json\n",
    "\n",
    "report = {\n",
    "    'model': MODEL_ID,\n",
    "    'date': '2026-02-06',\n",
    "    'experiment': 'sparsity_on_bitnet_2b',\n",
    "    'baseline_ppl': results[0]['perplexity'],\n",
    "    'results': [],\n",
    "}\n",
    "\n",
    "for r in results:\n",
    "    report['results'].append({\n",
    "        'target_sparsity': r['target_sparsity'],\n",
    "        'actual_sparsity': r['actual_sparsity'],\n",
    "        'perplexity': r['perplexity'],\n",
    "        'coherent': r['coherent'],\n",
    "        'sample_text': r['sample_text'],\n",
    "        'time_sec': r['time_sec'],\n",
    "    })\n",
    "\n",
    "with open('sparsity_bitnet_results.json', 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print('Resultados salvos em sparsity_bitnet_results.json')\n",
    "print()\n",
    "print('=== RESUMO FINAL ===')\n",
    "print('Modelo: {}'.format(MODEL_ID))\n",
    "print('PPL baseline: {:.2f}'.format(results[0]['perplexity']))\n",
    "print()\n",
    "for r in results:\n",
    "    delta = r['perplexity'] - results[0]['perplexity']\n",
    "    sign = '+' if delta > 0 else ''\n",
    "    status = 'OK' if r['coherent'] else 'QUEBROU'\n",
    "    print('  {:>3}% esparso -> PPL {:.2f} ({}{:.2f}) [{}]'.format(\n",
    "        r['target_sparsity'], r['perplexity'], sign, delta, status))"
   ]
  }
 ]
}