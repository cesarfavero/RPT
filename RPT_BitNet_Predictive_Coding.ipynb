{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# RPT - Validacao de Predictive Coding no BitNet 2B\n#\n# Hipotese: Residual connections em transformers sao codificacao preditiva.\n# Cada camada faz uma pequena correcao (output - input). Se essas correcoes\n# sao esparsas e podemos zerar as pequenas sem perder qualidade, validamos\n# que o modelo opera como codificacao preditiva.\n#\n# Diferenca dos testes anteriores:\n# - Antes: modificavamos PESOS (magnitude pruning)\n# - Agora: modificamos ATIVACOES durante inferencia (residual pruning via hooks)\n# - Nao precisa fine-tune! Cada teste e reversivel (remove hook)\n#\n# V2: Pruning por PERCENTIL (zerar os X% menores de cada camada)\n# V1 usava thresholds absolutos que nao estressavam o modelo (magnitudes 28-1199)\n#\n# IMPORTANTE: Rode Cell 1, REINICIE o runtime, rode Cell 1 de novo e continue."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CELL 1: SETUP\n",
    "!pip install -q torch torchvision\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git accelerate datasets\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "import collections\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import transformers\n",
    "\n",
    "print('Transformers:', transformers.__version__)\n",
    "print('Torch:', torch.__version__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "if device.type == 'cuda':\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "    mem = torch.cuda.get_device_properties(0).total_memory\n",
    "    print('VRAM: {:.1f} GB'.format(mem / 1e9))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# CELL 2: CARREGAR MODELO\nMODEL_ID = 'microsoft/bitnet-b1.58-2B-4T-bf16'\n\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\ntorch.set_float32_matmul_precision('high')\n\nprint('Carregando tokenizer...')\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprint('Carregando modelo BitNet 2B...')\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    dtype=torch.bfloat16,\n    device_map='auto'\n)\nmodel.eval()\n\nn_params = sum(p.numel() for p in model.parameters())\nprint('Parametros: {:,.0f} ({:.1f}B)'.format(n_params, n_params / 1e9))\n\n# Inspecionar arquitetura\nprint('\\n=== ARQUITETURA ===')\nnum_layers = model.config.num_hidden_layers\nhidden_size = model.config.hidden_size\nprint('Camadas: {}'.format(num_layers))\nprint('Hidden size: {}'.format(hidden_size))\n\n# Detectar nomes das camadas (robusto)\nmodules_dict = dict(model.named_modules())\nlayer_names = []\nfor i in range(num_layers):\n    # Tentar padroes comuns de HuggingFace\n    for pattern in ['model.layers.{}', 'transformer.h.{}', 'gpt_neox.layers.{}']:\n        name = pattern.format(i)\n        if name in modules_dict:\n            layer_names.append(name)\n            break\n\nprint('Blocos transformer encontrados: {}'.format(len(layer_names)))\nif layer_names:\n    print('  Primeiro: {}'.format(layer_names[0]))\n    print('  Ultimo: {}'.format(layer_names[-1]))\nelse:\n    print('  ERRO: Nenhum bloco encontrado! Listando modulos de nivel 2:')\n    for name in modules_dict:\n        if name.count('.') == 2:\n            print('    {}'.format(name))\n\n# Teste rapido: verificar que hooks vao funcionar\nif layer_names:\n    test_module = modules_dict[layer_names[0]]\n    print('  Tipo do bloco: {}'.format(type(test_module).__name__))\n\nif device.type == 'cuda':\n    print('VRAM usada: {:.1f} GB'.format(torch.cuda.memory_allocated() / 1e9))\nprint('Pronto!')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CELL 3: DATASET + FUNCOES BASE\n",
    "from datasets import load_dataset\n",
    "\n",
    "print('Carregando WikiText-2 validacao...')\n",
    "val_dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='validation')\n",
    "\n",
    "SEQ_LEN = 128\n",
    "val_ids = []\n",
    "for example in val_dataset:\n",
    "    text = example['text'].strip()\n",
    "    if len(text) < 20:\n",
    "        continue\n",
    "    val_ids.extend(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "val_chunks = []\n",
    "for i in range(0, len(val_ids) - SEQ_LEN, SEQ_LEN):\n",
    "    val_chunks.append(torch.tensor(val_ids[i:i + SEQ_LEN], dtype=torch.long))\n",
    "\n",
    "print('Val chunks: {:,}'.format(len(val_chunks)))\n",
    "\n",
    "# Textos para analise de ativacoes\n",
    "sample_texts = []\n",
    "for example in val_dataset:\n",
    "    text = example['text'].strip()\n",
    "    if len(text) > 100:\n",
    "        sample_texts.append(text[:500])\n",
    "    if len(sample_texts) >= 20:\n",
    "        break\n",
    "print('Textos para analise: {}'.format(len(sample_texts)))\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    'The capital of France is',\n",
    "    'Water boils at',\n",
    "    'The largest planet in the solar system is',\n",
    "]\n",
    "\n",
    "\n",
    "def compute_ppl(model, val_chunks, max_batches=50):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    for chunk in val_chunks[:max_batches]:\n",
    "        input_ids = chunk.unsqueeze(0).to(model.device if hasattr(model, 'device') else 'cuda')\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids, labels=input_ids)\n",
    "        total_loss += out.loss.item() * (chunk.shape[0] - 1)\n",
    "        total_tokens += chunk.shape[0] - 1\n",
    "    return torch.exp(torch.tensor(total_loss / total_tokens)).item()\n",
    "\n",
    "\n",
    "def test_gen(model, tokenizer, prompts):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    for p in prompts:\n",
    "        inp = tokenizer(p, return_tensors='pt').to(model.device if hasattr(model, 'device') else 'cuda')\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **inp, max_new_tokens=25,\n",
    "                do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "        results.append(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "    return results\n",
    "\n",
    "\n",
    "print('Funcoes definidas.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# CELL 4: CAPTURAR ATIVACOES (FORWARD HOOKS)\n\n# Cache do dict de modulos (criado uma vez so)\n_modules_dict = dict(model.named_modules())\n\n\ndef _get_module(name):\n    \"\"\"Busca modulo pelo nome no cache.\"\"\"\n    return _modules_dict.get(name)\n\n\nclass ResidualCapture:\n    \"\"\"Captura residual updates de cada camada do transformer.\n    \n    Em transformers: output = input + correction\n    A 'correction' e a correcao preditiva (attn + mlp).\n    Predictive coding diz que essa correcao deve ser esparsa.\n    \"\"\"\n    \n    def __init__(self):\n        self.hooks = []\n        self.stats = {}\n    \n    def register(self, layer_names):\n        for layer_name in layer_names:\n            module = _get_module(layer_name)\n            if module is None:\n                print('  AVISO: modulo {} nao encontrado'.format(layer_name))\n                continue\n            self.stats[layer_name] = []\n            handle = module.register_forward_hook(self._make_hook(layer_name))\n            self.hooks.append(handle)\n        print('Hooks registrados: {}'.format(len(self.hooks)))\n    \n    def _make_hook(self, layer_name):\n        stats_list = self.stats[layer_name]\n        def hook_fn(module, input, output):\n            x_in = input[0] if isinstance(input, tuple) else input\n            x_out = output[0] if isinstance(output, tuple) else output\n            \n            if not isinstance(x_in, torch.Tensor) or not isinstance(x_out, torch.Tensor):\n                return\n            if x_in.shape != x_out.shape:\n                return\n            \n            residual = (x_out - x_in).detach()\n            r_abs = residual.abs()\n            \n            stats_list.append({\n                'mean': r_abs.mean().item(),\n                'std': r_abs.std().item(),\n                'max': r_abs.max().item(),\n                'pct_zero': (r_abs == 0).float().mean().item() * 100,\n                'pct_below_001': (r_abs < 0.001).float().mean().item() * 100,\n                'pct_below_01': (r_abs < 0.01).float().mean().item() * 100,\n                'pct_below_05': (r_abs < 0.05).float().mean().item() * 100,\n                'pct_below_1': (r_abs < 0.1).float().mean().item() * 100,\n            })\n        return hook_fn\n    \n    def get_summary(self):\n        summary = {}\n        for layer_name, stats_list in self.stats.items():\n            if not stats_list:\n                continue\n            keys = stats_list[0].keys()\n            avg = {}\n            for k in keys:\n                vals = [s[k] for s in stats_list]\n                avg[k] = sum(vals) / len(vals)\n            avg['n_samples'] = len(stats_list)\n            summary[layer_name] = avg\n        return summary\n    \n    def remove(self):\n        for h in self.hooks:\n            h.remove()\n        self.hooks.clear()\n        self.stats.clear()\n\n\nclass ResidualPercentilePruner:\n    \"\"\"Aplica pruning por PERCENTIL nas ativacoes durante inferencia.\n    \n    Em vez de threshold absoluto fixo, zera os X% menores de cada camada.\n    Isso garante que realmente estressamos o modelo independente da escala.\n    \n    Exemplo: percentile=70 -> zera os 70% menores, mantem os 30% maiores.\n    \"\"\"\n    \n    def __init__(self):\n        self.hooks = []\n        self.total_activations = 0\n        self.total_pruned = 0\n    \n    def apply(self, layer_names, percentile):\n        self.total_activations = 0\n        self.total_pruned = 0\n        \n        for layer_name in layer_names:\n            module = _get_module(layer_name)\n            if module is None:\n                continue\n            handle = module.register_forward_hook(self._make_prune_hook(percentile))\n            self.hooks.append(handle)\n    \n    def _make_prune_hook(self, percentile):\n        pruner = self\n        q = percentile / 100.0\n        \n        def hook_fn(module, input, output):\n            x_in = input[0] if isinstance(input, tuple) else input\n            \n            if isinstance(output, tuple):\n                x_out = output[0]\n                rest = output[1:]\n            else:\n                x_out = output\n                rest = None\n            \n            if not isinstance(x_in, torch.Tensor) or not isinstance(x_out, torch.Tensor):\n                return output\n            if x_in.shape != x_out.shape:\n                return output\n            \n            residual = x_out - x_in\n            r_abs = residual.abs()\n            \n            # Threshold dinamico: percentil da distribuicao DESTA camada NESTE forward\n            threshold = torch.quantile(r_abs.float().flatten(), q)\n            \n            mask = (r_abs >= threshold).to(residual.dtype)\n            \n            pruner.total_activations += r_abs.numel()\n            pruner.total_pruned += (mask == 0).sum().item()\n            \n            new_out = x_in + residual * mask\n            \n            if rest is not None:\n                return (new_out,) + rest\n            return new_out\n        return hook_fn\n    \n    def get_pruned_pct(self):\n        if self.total_activations == 0:\n            return 0.0\n        return 100.0 * self.total_pruned / self.total_activations\n    \n    def reset_counters(self):\n        self.total_activations = 0\n        self.total_pruned = 0\n    \n    def remove(self):\n        for h in self.hooks:\n            h.remove()\n        self.hooks.clear()\n\n\n# Verificar que hooks funcionam com um forward pass de teste\nprint('Testando hooks...')\ncapture_test = ResidualCapture()\ncapture_test.register(layer_names)\n\ntest_input = tokenizer('Hello world', return_tensors='pt')\ntest_input = {k: v.to('cuda') for k, v in test_input.items()}\nwith torch.no_grad():\n    _ = model(**test_input)\n\ntest_summary = capture_test.get_summary()\ncapture_test.remove()\n\nif test_summary:\n    first_layer = list(test_summary.keys())[0]\n    print('  OK! {} camadas capturadas. Layer 0 mean residual: {:.4f}'.format(\n        len(test_summary), test_summary[first_layer]['mean']))\nelse:\n    print('  ERRO: Hooks nao capturaram nada! Verificar nomes das camadas.')\n\nprint('Classes definidas: ResidualCapture, ResidualPercentilePruner')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# CELL 5: BASELINE - ESPARSIDADE NATURAL DAS ATIVACOES\n\nprint('=== BASELINE: ESPARSIDADE NATURAL DAS ATIVACOES ===')\nprint('Capturando residual updates de {} camadas em {} textos...'.format(\n    len(layer_names), len(sample_texts)))\nprint()\n\nppl_baseline = compute_ppl(model, val_chunks)\nprint('PPL baseline: {:.2f}'.format(ppl_baseline))\nprint()\n\ncapture = ResidualCapture()\ncapture.register(layer_names)\n\nwith torch.no_grad():\n    for i, text in enumerate(sample_texts):\n        inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=256)\n        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n        _ = model(**inputs)\n        if (i + 1) % 5 == 0:\n            print('  Processados {}/{} textos'.format(i + 1, len(sample_texts)))\n\nsummary = capture.get_summary()\ncapture.remove()\n\nif not summary:\n    print('ERRO: Nenhuma ativacao capturada! Hooks nao funcionaram.')\nelse:\n    print()\n    print('{:<25} {:>8} {:>8} {:>8} {:>8} {:>8}'.format(\n        'Camada', 'Mean', '<0.001', '<0.01', '<0.05', '<0.1'))\n    print('-' * 75)\n\n    layer_indices = []\n    means = []\n    below_01 = []\n\n    for layer_name in layer_names:\n        if layer_name not in summary:\n            continue\n        s = summary[layer_name]\n        idx = int(layer_name.split('.')[-1])\n        layer_indices.append(idx)\n        means.append(s['mean'])\n        below_01.append(s['pct_below_01'])\n        print('{:<25} {:>8.4f} {:>7.1f}% {:>7.1f}% {:>7.1f}% {:>7.1f}%'.format(\n            'Layer {}'.format(idx),\n            s['mean'],\n            s['pct_below_001'],\n            s['pct_below_01'],\n            s['pct_below_05'],\n            s['pct_below_1']))\n\n    print()\n    print('=== ANALISE ===')\n\n    if len(means) > 1:\n        first_half = sum(means[:len(means)//2]) / (len(means)//2)\n        second_half = sum(means[len(means)//2:]) / (len(means) - len(means)//2)\n        print('Magnitude media (primeiras {} camadas): {:.4f}'.format(len(means)//2, first_half))\n        print('Magnitude media (ultimas {} camadas):   {:.4f}'.format(len(means) - len(means)//2, second_half))\n        if second_half < first_half:\n            print('-> Camadas profundas tem correcoes MENORES (confirma predictive coding)')\n        else:\n            print('-> Camadas profundas tem correcoes MAIORES (nao confirma predicao)')\n\n    avg_below_01 = sum(below_01) / len(below_01) if below_01 else 0\n    print('\\nMedia de ativacoes < 0.01: {:.1f}%'.format(avg_below_01))\n    if avg_below_01 > 10:\n        print('-> Ativacoes sao naturalmente esparsas! Predictive coding plausivel.')\n    else:\n        print('-> Ativacoes nao sao muito esparsas naturalmente.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CELL 6: BASELINE GERACAO\n",
    "\n",
    "print('=== BASELINE GERACAO ===')\n",
    "texts_baseline = test_gen(model, tokenizer, TEST_PROMPTS)\n",
    "for t in texts_baseline:\n",
    "    print('  {}'.format(t))\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print('\\nVRAM: {:.1f} GB'.format(torch.cuda.memory_allocated() / 1e9))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# CELL 7: TESTE PROGRESSIVO - PRUNING POR PERCENTIL\n#\n# Teste anterior (threshold absoluto) mostrou que magnitudes sao enormes (28-1199),\n# entao thresholds ate 0.5 so zeraram 0.6% das ativacoes.\n#\n# Agora: zerar os X% MENORES de cada camada, independente da escala.\n# percentile=70 -> zera os 70% menores, mantem os 30% maiores.\n# Como ~50% ja sao naturalmente zero, percentile=50 e ~baseline.\n\nPERCENTILES = [50, 60, 70, 75, 80, 85, 90, 95, 99]\n\nresults = []\nresults.append({\n    'percentile': 0,\n    'pct_pruned': 0.0,\n    'ppl': ppl_baseline,\n    'sample': texts_baseline[0],\n    'time_sec': 0,\n})\n\nprint('=== RESIDUAL PRUNING POR PERCENTIL ===')\nprint('Percentis: {}'.format(PERCENTILES))\nprint('percentile=X -> zera os X% menores de cada camada')\nprint('~50% das ativacoes ja sao naturalmente zero')\nprint()\n\nfor pct in PERCENTILES:\n    print('=' * 50)\n    print('PERCENTIL: {}% (manter top {}%)'.format(pct, 100 - pct))\n    t0 = time.time()\n    \n    # Aplicar hooks\n    pruner = ResidualPercentilePruner()\n    pruner.apply(layer_names, pct)\n    \n    # Medir PPL\n    print('  Medindo PPL...')\n    ppl = compute_ppl(model, val_chunks)\n    pct_pruned = pruner.get_pruned_pct()\n    \n    # Reset contadores antes de gerar texto\n    pruner.reset_counters()\n    \n    # Gerar texto\n    print('  Gerando texto...')\n    texts = test_gen(model, tokenizer, TEST_PROMPTS)\n    \n    # Remover hooks\n    pruner.remove()\n    \n    dt = time.time() - t0\n    \n    result = {\n        'percentile': pct,\n        'pct_pruned': pct_pruned,\n        'ppl': ppl,\n        'texts': texts,\n        'sample': texts[0],\n        'time_sec': dt,\n    }\n    results.append(result)\n    \n    delta = 100 * (ppl / ppl_baseline - 1)\n    print('  P{}: {:.1f}% zerado | PPL {:.2f} ({:+.1f}% vs base) | {:.0f}s'.format(\n        pct, pct_pruned, ppl, delta, dt))\n    for j, t in enumerate(texts):\n        print('  [{}] {}'.format(j + 1, t[:80]))\n    print()\n    \n    # Se PPL ficou 10x pior, parar\n    if ppl > ppl_baseline * 10:\n        print('  *** PPL {:.1f}x baseline, parando. ***'.format(ppl / ppl_baseline))\n        break\n\nprint('Finalizado.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# CELL 8: RESULTADOS + COMPARACAO\n\nprint('=' * 80)\nprint('RESULTADOS: PREDICTIVE CODING (PERCENTILE PRUNING) NO BITNET 2B')\nprint('=' * 80)\nprint()\nprint('{:<12} {:<12} {:<12} {:<12} {}'.format(\n    'Percentil', '% Zerado', 'PPL', 'vs Base', 'Texto'))\nprint('-' * 80)\n\nfor r in results:\n    if r['percentile'] == 0:\n        delta_str = '-'\n        label = '0 (base)'\n    else:\n        delta = 100 * (r['ppl'] / ppl_baseline - 1)\n        delta_str = '{:+.1f}%'.format(delta)\n        label = 'P{}'.format(r['percentile'])\n    sample = r['sample'][:35].replace('\\n', ' ')\n    print('{:<12} {:<12} {:<12} {:<12} {}'.format(\n        label,\n        '{:.1f}%'.format(r['pct_pruned']),\n        '{:.2f}'.format(r['ppl']),\n        delta_str,\n        sample))\n\nprint()\nprint('=== COMPARACAO: PESO vs ATIVACAO ===')\nprint()\nprint('Esparsidade de PESOS (magnitude pruning + fine-tune):')\nprint('  10% pesos zerados = PPL -40.4% (MELHOR)')\nprint('  40% pesos zerados = PPL -29.3% (maximo usavel)')\nprint()\nprint('Esparsidade de ATIVACOES (residual pruning, SEM fine-tune):')\n\nbest = min(results, key=lambda x: x['ppl'])\nprint('  Melhor: P{} -> {:.1f}% zerado, PPL {:.2f} ({:+.1f}%)'.format(\n    best['percentile'],\n    best['pct_pruned'],\n    best['ppl'],\n    100 * (best['ppl'] / ppl_baseline - 1)))\n\nusable = [r for r in results if r['ppl'] < ppl_baseline * 1.5 and r['percentile'] > 0]\nif usable:\n    most_pruned = max(usable, key=lambda x: x['pct_pruned'])\n    print('  Max usavel (<50% PPL): P{} -> {:.1f}% zerado, PPL {:.2f}'.format(\n        most_pruned['percentile'],\n        most_pruned['pct_pruned'],\n        most_pruned['ppl']))\n\n# Encontrar onde degrada significativamente (>10% PPL)\ndegraded = [r for r in results if r['ppl'] > ppl_baseline * 1.1 and r['percentile'] > 0]\nif degraded:\n    first_bad = min(degraded, key=lambda x: x['percentile'])\n    print('  Degrada (>10% PPL) a partir de: P{}'.format(first_bad['percentile']))\n\nprint()\nprint('=== CONCLUSAO ===')\nif best['ppl'] < ppl_baseline * 0.99:\n    print('PREDICTIVE CODING VALIDADO: zerar correcoes pequenas MELHORA o modelo!')\nelif usable and most_pruned['pct_pruned'] > 60:\n    print('PREDICTIVE CODING VALIDADO: modelo tolera {:.0f}% de ativacoes zeradas'.format(\n        most_pruned['pct_pruned']))\n    print('Correcoes residuais sao redundantes - confirma codificacao preditiva.')\nelif usable and most_pruned['pct_pruned'] > 50:\n    print('PREDICTIVE CODING PARCIAL: modelo tolera ate {:.0f}% zerado'.format(\n        most_pruned['pct_pruned']))\n    print('Alguma redundancia nas correcoes, mas limitada.')\nelse:\n    print('PREDICTIVE CODING NAO CONFIRMADO: modelo sensivel a mudancas nas ativacoes.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# CELL 9: SALVAR RESULTADOS\n\nreport = {\n    'model': MODEL_ID,\n    'date': '2026-02-06',\n    'experiment': 'predictive_coding_percentile_pruning_bitnet_2b',\n    'config': {\n        'gpu': torch.cuda.get_device_name(0) if device.type == 'cuda' else 'cpu',\n        'num_layers': len(layer_names),\n        'seq_len': SEQ_LEN,\n        'num_sample_texts': len(sample_texts),\n        'dataset': 'wikitext-2',\n        'percentiles_tested': PERCENTILES,\n    },\n    'natural_sparsity': {},\n    'baseline_ppl': ppl_baseline,\n    'results': [],\n}\n\nfor layer_name, s in summary.items():\n    report['natural_sparsity'][layer_name] = s\n\nfor r in results:\n    entry = {\n        'percentile': r['percentile'],\n        'pct_pruned': r['pct_pruned'],\n        'ppl': r['ppl'],\n        'sample_text': r['sample'],\n        'time_sec': r['time_sec'],\n    }\n    if 'texts' in r:\n        entry['all_texts'] = r['texts']\n    report['results'].append(entry)\n\nfilename = 'predictive_coding_percentile_bitnet2b_results.json'\nwith open(filename, 'w') as f:\n    json.dump(report, f, indent=2)\n\nprint('Salvo em {}'.format(filename))\nprint()\nprint('=== RESUMO FINAL ===')\nprint('Baseline PPL: {:.2f}'.format(ppl_baseline))\nfor r in results:\n    if r['percentile'] == 0:\n        continue\n    delta = 100 * (r['ppl'] / ppl_baseline - 1)\n    print('  P{}: {:.1f}% zerado, PPL {:.2f} ({:+.1f}%)'.format(\n        r['percentile'], r['pct_pruned'], r['ppl'], delta))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# CELL 10: DOWNLOAD RESULTADOS\nimport os\n\nfilename = 'predictive_coding_percentile_bitnet2b_results.json'\nfilepath = os.path.abspath(filename)\nprint('Arquivo salvo em: {}'.format(filepath))\nprint('Tamanho: {:.1f} KB'.format(os.path.getsize(filepath) / 1024))\n\ntry:\n    from google.colab import files\n    files.download(filename)\n    print('Download Colab iniciado.')\nexcept ImportError:\n    print('Nao esta no Colab. Copie o arquivo manualmente do caminho acima.')",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}