{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# RPT - BitNet b1.58 2B-4T (Microsoft)\n#\n# Modelo oficial: 2B parametros, treinado do zero com pesos ternarios {-1, 0, +1}.\n# Usando versao bf16 (pesos desempacotados, ~4GB VRAM).\n#\n# IMPORTANTE: Rode Cell 1, depois REINICIE o runtime, depois rode Cell 1 de novo e continue."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# CELL 1: SETUP\n# Na primeira vez, o pip install roda. Reinicie o runtime e rode esta cell de novo.\n!pip install -q torch torchvision\n!pip install -q git+https://github.com/huggingface/transformers.git accelerate\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport transformers\n\nprint('Transformers:', transformers.__version__)\nprint('Torch:', torch.__version__)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Device:', device)\nif device.type == 'cuda':\n    print('GPU:', torch.cuda.get_device_name(0))\n    mem = torch.cuda.get_device_properties(0).total_memory\n    print('VRAM: {:.1f} GB'.format(mem / 1e9))\n    print('BF16 suportado:', torch.cuda.is_bf16_supported())",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# CELL 2: CARREGAR MODELO\nMODEL_ID = 'microsoft/bitnet-b1.58-2B-4T-bf16'\n\nprint('Carregando tokenizer...')\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n\nprint('Carregando modelo BitNet 2B...')\n# T4 suporta bfloat16 via emulacao (funciona, so e mais lento)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    dtype=torch.bfloat16,\n    device_map='auto'\n)\nmodel.eval()\n\nn_params = sum(p.numel() for p in model.parameters())\nprint('Parametros: {:,.0f} ({:.1f}B)'.format(n_params, n_params / 1e9))\nif device.type == 'cuda':\n    print('VRAM usada: {:.1f} GB'.format(torch.cuda.memory_allocated() / 1e9))\nprint('Pronto!')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# CELL 3: TESTE BASICO (completar frase)\nTEST_PROMPTS = [\n    'The capital of France is',\n    'Water boils at',\n    'The largest planet in the solar system is',\n    'Python is a programming language that',\n    'In 1969, humans first',\n]\n\nprint('=== COMPLETAR FRASE ===')\nfor prompt in TEST_PROMPTS:\n    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n    with torch.no_grad():\n        out = model.generate(\n            **inputs,\n            max_new_tokens=30,\n            do_sample=False,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    text = tokenizer.decode(out[0], skip_special_tokens=True)\n    print('  {}'.format(text))\n    print()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# CELL 4: TESTE CHAT\ndef chat(user_msg, system_msg='You are a helpful AI assistant.', max_tokens=200):\n    messages = [\n        {'role': 'system', 'content': system_msg},\n        {'role': 'user', 'content': user_msg},\n    ]\n    prompt = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n    input_len = inputs['input_ids'].shape[-1]\n    with torch.no_grad():\n        out = model.generate(\n            **inputs,\n            max_new_tokens=max_tokens,\n            do_sample=False,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    return tokenizer.decode(out[0][input_len:], skip_special_tokens=True)\n\nprint('=== TESTE CHAT ===')\nperguntas = [\n    'What is the capital of France?',\n    'Explain what BitNet is in 2 sentences.',\n    'Write a Python function that checks if a number is prime.',\n]\n\nfor p in perguntas:\n    print('User:', p)\n    print('BitNet:', chat(p))\n    print('-' * 60)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CELL 5: ARQUITETURA\n",
    "import collections\n",
    "\n",
    "print('=== ARQUITETURA ===')\n",
    "print('Modelo:', type(model).__name__)\n",
    "\n",
    "layer_types = collections.Counter()\n",
    "for name, m in model.named_modules():\n",
    "    layer_types[type(m).__name__] += 1\n",
    "\n",
    "print('\\nCamadas:')\n",
    "for ltype, count in layer_types.most_common(15):\n",
    "    print('  {:>4}x {}'.format(count, ltype))\n",
    "\n",
    "print('\\nPrimeiros parametros:')\n",
    "for i, (name, param) in enumerate(model.named_parameters()):\n",
    "    if i >= 10:\n",
    "        print('  ...')\n",
    "        break\n",
    "    print('  {} | {} | dtype={}'.format(\n",
    "        name[:55], list(param.shape), param.dtype))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CELL 6: MODO INTERATIVO\n",
    "print('=== MODO INTERATIVO ===')\n",
    "print('Digite uma pergunta (ou \"sair\"):')\n",
    "print()\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input('Voce > ').strip()\n",
    "        if not user_input or user_input.lower() in ('sair', 'exit', 'q'):\n",
    "            break\n",
    "        response = chat(user_input)\n",
    "        print('BitNet > {}'.format(response))\n",
    "        print()\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "print('Ate mais!')"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}