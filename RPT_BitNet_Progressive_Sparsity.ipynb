{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RPT - Pruning Progressivo com Fine-Tune no BitNet 2B\n",
    "#\n",
    "# Objetivo: empurrar esparsidade alem de 10% usando ciclos de poda+treino.\n",
    "# Base: RPT_BitNet_Microsoft.ipynb + RPT_BitNet_Sparsity_Test.ipynb\n",
    "#\n",
    "# Resultado anterior: 10% magnitude pruning cru -> PPL 6.94 (-26%)\n",
    "# Meta: chegar a 30-50%+ mantendo qualidade.\n",
    "#\n",
    "# IMPORTANTE: Rode Cell 1, REINICIE o runtime, rode Cell 1 de novo e continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: SETUP\n",
    "!pip install -q torch torchvision\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git accelerate datasets\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import transformers\n",
    "\n",
    "print('Transformers:', transformers.__version__)\n",
    "print('Torch:', torch.__version__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "if device.type == 'cuda':\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "    mem = torch.cuda.get_device_properties(0).total_memory\n",
    "    print('VRAM: {:.1f} GB'.format(mem / 1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 2: CARREGAR MODELO (H100 otimizado)\nMODEL_ID = 'microsoft/bitnet-b1.58-2B-4T-bf16'\n\n# Otimizacoes H100\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\ntorch.set_float32_matmul_precision('high')\n\nprint('Carregando tokenizer...')\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprint('Carregando modelo BitNet 2B...')\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    dtype=torch.bfloat16,\n    device_map='auto'\n)\n\n# torch.compile para otimizar kernels na H100\nprint('Compilando modelo (torch.compile)...')\nmodel = torch.compile(model)\n\nn_params = sum(p.numel() for p in model.parameters())\nprint('Parametros: {:,.0f} ({:.1f}B)'.format(n_params, n_params / 1e9))\nif device.type == 'cuda':\n    print('VRAM usada: {:.1f} GB'.format(torch.cuda.memory_allocated() / 1e9))\nprint('TF32 ativado: {}'.format(torch.backends.cuda.matmul.allow_tf32))\nprint('Pronto!')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 3: CARREGAR DATASET\nfrom datasets import load_dataset\n\nprint('Carregando WikiText-2...')\ndataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n\n# seq_len=128 (rapido por step)\nSEQ_LEN = 128\nprint('Tokenizando (seq_len={})...'.format(SEQ_LEN))\nall_ids = []\nfor example in dataset:\n    text = example['text'].strip()\n    if len(text) < 20:\n        continue\n    ids = tokenizer.encode(text, add_special_tokens=False)\n    all_ids.extend(ids)\n\nchunks = []\nfor i in range(0, len(all_ids) - SEQ_LEN, SEQ_LEN):\n    chunks.append(torch.tensor(all_ids[i:i + SEQ_LEN], dtype=torch.long))\n\nprint('Tokens totais: {:,}'.format(len(all_ids)))\nprint('Chunks de {}: {:,}'.format(SEQ_LEN, len(chunks)))\n\nval_dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='validation')\nval_ids = []\nfor example in val_dataset:\n    text = example['text'].strip()\n    if len(text) < 20:\n        continue\n    val_ids.extend(tokenizer.encode(text, add_special_tokens=False))\n\nval_chunks = []\nfor i in range(0, len(val_ids) - SEQ_LEN, SEQ_LEN):\n    val_chunks.append(torch.tensor(val_ids[i:i + SEQ_LEN], dtype=torch.long))\n\nprint('Val chunks: {:,}'.format(len(val_chunks)))\nprint('Dataset pronto.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 4: FUNCOES (H100 - velocidade maxima)\n\nimport random\n\nTEST_PROMPTS = [\n    'The capital of France is',\n    'Water boils at',\n    'The largest planet in the solar system is',\n]\n\n\ndef compute_ppl(model, val_chunks, max_batches=50):\n    \"\"\"Perplexity no validation set.\"\"\"\n    model.eval()\n    total_loss = 0.0\n    total_tokens = 0\n    batches = val_chunks[:max_batches]\n    for chunk in batches:\n        input_ids = chunk.unsqueeze(0).to(model.device if hasattr(model, 'device') else 'cuda')\n        with torch.no_grad():\n            out = model(input_ids=input_ids, labels=input_ids)\n        total_loss += out.loss.item() * (chunk.shape[0] - 1)\n        total_tokens += chunk.shape[0] - 1\n    return torch.exp(torch.tensor(total_loss / total_tokens)).item()\n\n\ndef test_gen(model, tokenizer, prompts):\n    \"\"\"Gera texto e retorna lista.\"\"\"\n    model.eval()\n    results = []\n    for p in prompts:\n        inp = tokenizer(p, return_tensors='pt').to(model.device if hasattr(model, 'device') else 'cuda')\n        with torch.no_grad():\n            out = model.generate(\n                **inp, max_new_tokens=25,\n                do_sample=False, pad_token_id=tokenizer.eos_token_id)\n        results.append(tokenizer.decode(out[0], skip_special_tokens=True))\n    return results\n\n\ndef prune_magnitude(model, sparsity_pct, protect_layers=None):\n    \"\"\"Poda por magnitude global. Retorna esparsidade real.\"\"\"\n    if protect_layers is None:\n        protect_layers = ['embed', 'lm_head']\n\n    samples = []\n    total_nonzero = 0\n    for name, param in model.named_parameters():\n        if param.dim() < 2:\n            continue\n        if any(p in name for p in protect_layers):\n            continue\n        w = param.data.cpu().float()\n        nonzero = w[w != 0].abs()\n        n = nonzero.numel()\n        if n == 0:\n            continue\n        total_nonzero += n\n        if n > 50000:\n            idx = torch.randperm(n)[:50000]\n            samples.append(nonzero[idx])\n        else:\n            samples.append(nonzero)\n\n    sample = torch.cat(samples)\n    sample_sorted = sample.sort().values\n    idx = min(int(len(sample_sorted) * sparsity_pct / 100.0), len(sample_sorted) - 1)\n    threshold = sample_sorted[idx].item()\n    del sample, sample_sorted, samples\n\n    total_pruned = 0\n    total_weights = 0\n    for name, param in model.named_parameters():\n        if param.dim() < 2:\n            continue\n        if any(p in name for p in protect_layers):\n            continue\n        w = param.data.cpu().float()\n        mask = w.abs() > threshold\n        param.data.copy_((w * mask.float()).to(param.dtype).to(param.device))\n        total_pruned += (~mask).sum().item()\n        total_weights += w.numel()\n\n    actual = 100.0 * total_pruned / total_weights if total_weights > 0 else 0\n    print('    [prune] {:.1f}% zerados ({:,}/{:,})'.format(actual, total_pruned, total_weights))\n    return actual\n\n\ndef finetune(model, chunks, n_steps=300, lr=5e-4, batch_size=64):\n    \"\"\"Fine-tune com AdamW - batch grande para saturar H100.\"\"\"\n    model.train()\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_steps)\n\n    losses = []\n    t0 = time.time()\n    for step in range(n_steps):\n        batch_chunks = random.choices(chunks, k=batch_size)\n        input_ids = torch.stack(batch_chunks).to(model.device if hasattr(model, 'device') else 'cuda')\n\n        out = model(input_ids=input_ids, labels=input_ids)\n        loss = out.loss\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n        losses.append(loss.item())\n\n        if (step + 1) % 50 == 0:\n            avg = sum(losses[-50:]) / len(losses[-50:])\n            elapsed = time.time() - t0\n            steps_per_sec = (step + 1) / elapsed\n            eta = (n_steps - step - 1) / steps_per_sec\n            print('    [ft] {}/{} | loss: {:.3f} | {:.1f} steps/s | ETA: {:.0f}s'.format(\n                step + 1, n_steps, avg, steps_per_sec, eta))\n\n    model.eval()\n    optimizer.zero_grad(set_to_none=True)\n    del optimizer, scheduler\n    torch.cuda.empty_cache()\n\n    final_loss = sum(losses[-50:]) / len(losses[-50:])\n    total_time = time.time() - t0\n    print('    [ft] Pronto em {:.0f}s | Loss final: {:.3f}'.format(total_time, final_loss))\n    return final_loss\n\n\nprint('Funcoes definidas (H100 max speed).')\nprint('batch=64 x seq=128 = 8K tokens/step')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5: BASELINE\n",
    "\n",
    "print('=== BASELINE ===')\n",
    "ppl_baseline = compute_ppl(model, val_chunks)\n",
    "print('PPL baseline: {:.2f}'.format(ppl_baseline))\n",
    "\n",
    "texts_baseline = test_gen(model, tokenizer, TEST_PROMPTS)\n",
    "for t in texts_baseline:\n",
    "    print('  {}'.format(t))\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print('VRAM: {:.1f} GB'.format(torch.cuda.memory_allocated() / 1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 6: PRUNING PROGRESSIVO COM FINE-TUNE (H100 - MAX SPEED)\n# batch=64 usa a VRAM da H100, 300 steps basta (mais dados por step)\n\nSPARSITY_STEPS = [5, 10, 15, 20, 25, 30, 40, 50]\nFT_STEPS = 300       # menos steps (cada step ve 64 chunks)\nFT_LR = 5e-4\nBATCH_SIZE = 64      # usa a VRAM da H100\n\nresults = []\n\nresults.append({\n    'sparsity': 0,\n    'actual_sparsity': 0.0,\n    'ppl_before_ft': ppl_baseline,\n    'ppl_after_ft': ppl_baseline,\n    'ft_loss': 0,\n    'sample': texts_baseline[0],\n    'time_sec': 0,\n})\n\ntokens_per_level = FT_STEPS * BATCH_SIZE * SEQ_LEN\nprint('=== PRUNING PROGRESSIVO (H100 MAX SPEED) ===')\nprint('Fine-tune: {} steps x batch {} x seq {} = {:.1f}M tokens/nivel'.format(\n    FT_STEPS, BATCH_SIZE, SEQ_LEN, tokens_per_level / 1e6))\nprint('AdamW lr={}'.format(FT_LR))\nprint()\n\nfor target_sp in SPARSITY_STEPS:\n    print('=' * 50)\n    print('NIVEL: {}%'.format(target_sp))\n    t0 = time.time()\n\n    print('  [1/4] Podando...')\n    actual_sp = prune_magnitude(model, target_sp)\n\n    print('  [2/4] PPL antes...')\n    ppl_before = compute_ppl(model, val_chunks)\n    print('    {:.2f}'.format(ppl_before))\n\n    print('  [3/4] Fine-tuning...')\n    ft_loss = finetune(model, chunks, n_steps=FT_STEPS, lr=FT_LR, batch_size=BATCH_SIZE)\n\n    print('  [4/4] PPL depois + texto...')\n    ppl_after = compute_ppl(model, val_chunks)\n    texts = test_gen(model, tokenizer, TEST_PROMPTS)\n\n    dt = time.time() - t0\n\n    result = {\n        'sparsity': target_sp,\n        'actual_sparsity': actual_sp,\n        'ppl_before_ft': ppl_before,\n        'ppl_after_ft': ppl_after,\n        'ft_loss': ft_loss,\n        'sample': texts[0],\n        'time_sec': dt,\n    }\n    results.append(result)\n\n    recovery = ppl_before - ppl_after\n    print()\n    print('  {}%: PPL {:.2f} -> {:.2f} (recuperou {:.2f}) | vs base: {:+.1f}% | {:.0f}s'.format(\n        target_sp, ppl_before, ppl_after, recovery,\n        100 * (ppl_after / ppl_baseline - 1), dt))\n    print('  Texto: {}'.format(texts[0][:80]))\n    print()\n\n    if ppl_after > ppl_baseline * 3:\n        print('  *** PPL {:.1f}x baseline, parando. ***'.format(ppl_after / ppl_baseline))\n        break\n\nprint('Finalizado.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7: TABELA DE RESULTADOS\n",
    "\n",
    "print('=' * 80)\n",
    "print('RESULTADOS: PRUNING PROGRESSIVO COM FINE-TUNE NO BITNET 2B')\n",
    "print('=' * 80)\n",
    "print()\n",
    "print('{:<10} {:<10} {:<12} {:<12} {:<10} {}'.format(\n",
    "    'Sparse%', 'Real%', 'PPL antes', 'PPL depois', 'vs Base', 'Texto'))\n",
    "print('-' * 80)\n",
    "\n",
    "for r in results:\n",
    "    if r['sparsity'] == 0:\n",
    "        delta_str = '-'\n",
    "    else:\n",
    "        delta = 100 * (r['ppl_after_ft'] / ppl_baseline - 1)\n",
    "        delta_str = '{:+.1f}%'.format(delta)\n",
    "    sample = r['sample'][:35].replace('\\n', ' ')\n",
    "    print('{:<10} {:<10} {:<12} {:<12} {:<10} {}'.format(\n",
    "        '{}%'.format(r['sparsity']),\n",
    "        '{:.1f}%'.format(r['actual_sparsity']),\n",
    "        '{:.2f}'.format(r['ppl_before_ft']),\n",
    "        '{:.2f}'.format(r['ppl_after_ft']),\n",
    "        delta_str,\n",
    "        sample))\n",
    "\n",
    "print()\n",
    "print('--- COMPARACAO ---')\n",
    "print('Baseline (0%): PPL {:.2f}'.format(ppl_baseline))\n",
    "print('Pruning cru 10% (sem fine-tune): PPL 6.94')\n",
    "print()\n",
    "\n",
    "# Melhor resultado\n",
    "best = min(results, key=lambda x: x['ppl_after_ft'])\n",
    "print('MELHOR: {}% esparsidade -> PPL {:.2f} ({:+.1f}% vs baseline)'.format(\n",
    "    best['sparsity'],\n",
    "    best['ppl_after_ft'],\n",
    "    100 * (best['ppl_after_ft'] / ppl_baseline - 1)))\n",
    "\n",
    "# Maximo esparso que ainda e melhor que baseline\n",
    "better = [r for r in results if r['ppl_after_ft'] <= ppl_baseline and r['sparsity'] > 0]\n",
    "if better:\n",
    "    most_sparse = max(better, key=lambda x: x['sparsity'])\n",
    "    print('MAX esparsidade melhor que baseline: {}% (PPL {:.2f})'.format(\n",
    "        most_sparse['sparsity'], most_sparse['ppl_after_ft']))\n",
    "else:\n",
    "    within = [r for r in results if r['ppl_after_ft'] <= ppl_baseline * 1.1 and r['sparsity'] > 0]\n",
    "    if within:\n",
    "        most_sparse = max(within, key=lambda x: x['sparsity'])\n",
    "        print('MAX esparsidade dentro de 10% do baseline: {}% (PPL {:.2f})'.format(\n",
    "            most_sparse['sparsity'], most_sparse['ppl_after_ft']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8: TESTE DETALHADO DO MELHOR NIVEL\n",
    "\n",
    "def chat(user_msg, system_msg='You are a helpful AI assistant.', max_tokens=200):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_msg},\n",
    "        {'role': 'user', 'content': user_msg},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    input_len = inputs['input_ids'].shape[-1]\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(out[0][input_len:], skip_special_tokens=True)\n",
    "\n",
    "# Pegar esparsidade atual do modelo (ultimo nivel treinado)\n",
    "current_sp = results[-1]['sparsity']\n",
    "current_ppl = results[-1]['ppl_after_ft']\n",
    "\n",
    "print('=== TESTE DETALHADO: {}% ESPARSIDADE (com fine-tune) ==='.format(current_sp))\n",
    "print('PPL: {:.2f}'.format(current_ppl))\n",
    "print()\n",
    "\n",
    "all_prompts = [\n",
    "    'The capital of France is',\n",
    "    'Water boils at',\n",
    "    'The largest planet in the solar system is',\n",
    "    'Python is a programming language that',\n",
    "    'In 1969, humans first',\n",
    "]\n",
    "\n",
    "print('--- Completar frase ---')\n",
    "for p in all_prompts:\n",
    "    inp = tokenizer(p, return_tensors='pt').to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inp, max_new_tokens=30,\n",
    "            do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "    print('  {}'.format(tokenizer.decode(out[0], skip_special_tokens=True)))\n",
    "print()\n",
    "\n",
    "print('--- Chat ---')\n",
    "perguntas = [\n",
    "    'What is the capital of France?',\n",
    "    'Explain what machine learning is in 2 sentences.',\n",
    "    'Write a Python function that checks if a number is prime.',\n",
    "]\n",
    "for p in perguntas:\n",
    "    print('User:', p)\n",
    "    print('BitNet ({}% sparse):'.format(current_sp), chat(p))\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CELL 9: SALVAR RESULTADOS\n\nreport = {\n    'model': MODEL_ID,\n    'date': '2026-02-06',\n    'experiment': 'progressive_pruning_finetune_bitnet_2b_h100',\n    'config': {\n        'gpu': 'H100',\n        'ft_steps_per_level': FT_STEPS,\n        'lr': FT_LR,\n        'batch_size': BATCH_SIZE,\n        'seq_len': SEQ_LEN,\n        'optimizer': 'AdamW',\n        'weight_decay': 0.01,\n        'scheduler': 'CosineAnnealingLR',\n        'torch_compile': True,\n        'tf32': True,\n        'dataset': 'wikitext-2',\n    },\n    'baseline_ppl': ppl_baseline,\n    'results': [],\n}\n\nfor r in results:\n    report['results'].append({\n        'sparsity': r['sparsity'],\n        'actual_sparsity': r['actual_sparsity'],\n        'ppl_before_ft': r['ppl_before_ft'],\n        'ppl_after_ft': r['ppl_after_ft'],\n        'ft_loss': r['ft_loss'],\n        'sample_text': r['sample'],\n        'time_sec': r['time_sec'],\n    })\n\nwith open('progressive_sparsity_results.json', 'w') as f:\n    json.dump(report, f, indent=2)\n\nprint('Salvo em progressive_sparsity_results.json')\nprint()\nprint('=== RESUMO FINAL ===')\nprint('Baseline PPL: {:.2f}'.format(ppl_baseline))\nprint()\nfor r in results:\n    if r['sparsity'] == 0:\n        continue\n    delta = 100 * (r['ppl_after_ft'] / ppl_baseline - 1)\n    print('  {}% -> PPL {:.2f} antes, {:.2f} depois ({:+.1f}% vs baseline) [{:.0f}s]'.format(\n        r['sparsity'], r['ppl_before_ft'], r['ppl_after_ft'], delta, r['time_sec']))"
  },
  {
   "cell_type": "code",
   "source": "# CELL 10: DOWNLOAD RESULTADOS\nimport os\n\nfilepath = os.path.abspath('progressive_sparsity_results.json')\nprint('Arquivo salvo em: {}'.format(filepath))\nprint('Tamanho: {:.1f} KB'.format(os.path.getsize(filepath) / 1024))\n\ntry:\n    from google.colab import files\n    files.download('progressive_sparsity_results.json')\n    print('Download Colab iniciado.')\nexcept ImportError:\n    print('Nao esta no Colab. Copie o arquivo manualmente do caminho acima.')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}